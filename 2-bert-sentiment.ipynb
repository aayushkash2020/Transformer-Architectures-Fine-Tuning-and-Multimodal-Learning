{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment Analysis Fine-Tuning with BERT\n",
    "\n",
    "In this part you will fine-tune a pre-trained encoder-only language model called Bert (originally trained and released by Google in 2018) for a sentiment analysis task. Unlike a causal GPT-style language model, BERT is bidirectional in the sense that it was trained to predict a masked word in the middle of a sequence using both the previous and subsequent tokens. For example, BERT was trained on tasks like predicting the masked token in `The sweet black cat [MASK] by the window in the sun.` considering both the preceding tokens `The sweet black cat` **and** the subsequent tokens `by the window in the sun.` \n",
    "\n",
    "This kind of model is not used for autoregressively generating new text, but is very useful when you want to understand an entire sequence of text as a whole, allowing attention to earlier or later tokens in a sequence. Sentiment analysis, wherein we want to classify an entire input sequence as either positive or negative in sentiment (for example, in this text we classify movie reviews as either positive or negative), is a good example where this kind of understanding is important.\n",
    "\n",
    "In this part we will directly modify the `PyTorch` model and will conduct the fine-tuning directly in `PyTorch` as we have done with previous models.\n",
    "\n",
    "**Learning objectives.** You will:\n",
    "1. Examine an encoder-only BERT transformer model\n",
    "2. Modify a BERT model for sentiment analysis\n",
    "3. Fine-tune the model on movie review data for sentiment analysis\n",
    "\n",
    "While it is possible to complete this assignment using CPU compute, it may be slow. To accelerate your training, consider using GPU resources such as `CUDA` through the CS department cluster. Alternatives include Google colab or local GPU resources for those running on machines with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, ensure that you have the `transformers` and `datasets` modules installed. We will use these modules for importing tokenizers, pretrained models, and datasets. You can run the following cells to try to install them with `pip` if needed. If you are using ondemand, ideally you would simply include `module load transformers` and `module load datasets` when making your initial reservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.29.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the following code imports a *tokenizer* and demonstrates its use. \n",
    "\n",
    "Note how the sequence of words in the input string is replaced with a sequence of numbers in the `input_ids`: These are indices into the vocabulary of 30522 used by the tokenizer. Also note the `special_tokens`: an `[UNK]` is used for anything not in the vocabulary, and a `[PAD]` can be useful for padding out a sequence of tokens to a specified length.\n",
    "\n",
    "Given a sequence of strings, the tokenizer returns a dictionary containing not just the `input_ids` (what you will most often want to use) but also `token_type_ids` (whether the token is special, which you will use least often) and `attention_mask`. The `attention_mask` has the same dimensions as the `input_ids` with a `1` in a given position if there is a non-padding token in that position and a `0` if that position is just a padding token. This is helpful when you are tokenizing a batch of multiple strings with potentially different lengths but want to create a single tensor. `padding='longest'` as shown pads all of the input to the same number of tokens as the longest input by adding `[PAD]` tokens to the end. The `attention_mask` is then passed so that you can ignore the extraneous padding tokens as needed.\n",
    "\n",
    "Also note the `return_tensors` parameter. Using `\"pt\"` as shown indicates that the results should be returned as PyTorch tensor. If you omit this parameter then the results will be returned as a Python list by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "{'input_ids': tensor([[  101,  1996, 11190,   102,     0,     0],\n",
      "        [  101,  5598,  2058,  1996,  4231,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased',\n",
    "                                          clean_up_tokenization_spaces=True)\n",
    "print(tokenizer)\n",
    "tokenized = tokenizer([\"the cow\", \"jumped over the moon\"], padding='longest', return_tensors=\"pt\")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer also has a `decode` method by which you can translate `input_ids` back into strings. You can optionally set `skip_special_tokens=True` if you want to ignore the special tokens like padding, unknown, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cow\n",
      "jumped over the moon\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "for tokens in tokenized[\"input_ids\"]:\n",
    "    print(tokenizer.decode(tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import our language model, in this case a pretrained BERT model. This is an encoder-only transformer architecture previewed below. As you can see, the embedding expects a vocabulary of 30522 matching our tokenizer. The model embedding dimension is 768 and the output layer of the model also has 768 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "pretrained_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "pretrained_model.to(device)\n",
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Our goal will be to modify a base Bert model for a sentiment analysis task. Specifically, we want to predict whether a given review text has a positive (1) or negative (0) sentiment. Define a model architecture that uses the pretrained BERT model but modifies it for classifying a sequence as positive or negative.\n",
    "\n",
    "Before proceeding, create a model object and ensure you can run forward progagation on a small example such as that defined in the second code block below. Your values may not be interpretable yet prior to fine-tuning, but you should be able to generate outputs of the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: define a model architecture for sentiment analysis using BERT\n",
    "class SentimentBert(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super(SentimentBert, self).__init__()\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0725],\n",
      "        [-0.1148]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# todo: try inference with your architecture here\n",
    "tokenized = tokenizer([\"the cow\", \"jumped over the moon\"], padding='longest', return_tensors=\"pt\")\n",
    "model = SentimentBert()\n",
    "model.to(device)\n",
    "input_ids = tokenized[\"input_ids\"].to(device)\n",
    "attention_mask = tokenized[\"attention_mask\"].to(device)\n",
    "logits = model(input_ids, attention_mask)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Our dataset is drawn from several thousand reviews on the Rotten Tomatoes website. Below we download and preview some of the data. Note that each element of a dataset is a dictionary with a `text` containing the review and a `label` which is `1` for a positive review or `0` for a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 8530, Validation examples: 1066\n",
      "{'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'label': 1}\n",
      "{'text': 'things really get weird , though not particularly scary : the movie is all portent and no content .', 'label': 0}\n",
      "{'text': 'effective but too-tepid biopic', 'label': 1}\n",
      "{'text': 'interminably bleak , to say nothing of boring .', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "from datasets import load_dataset\n",
    "train_data = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
    "val_data = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}, Validation examples: {len(val_data)}\")\n",
    "for i in range(1, 3):\n",
    "    print(train_data[i])\n",
    "    print(train_data[-i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the reviews are not all the same length. It is better not to pad the entire dataset to the same length, and instead just to perform padding per batch. We will want to have `DataLoader`s for easy iteration over batches of data as tokenized tensors. \n",
    "\n",
    "One way to do this is to supply a `collate_fn` to the `DataLoader` constructor. This is a function that takes as input a list of elements from the dataset (called `batch`), which in our case will be a list of dictionaries containing `text` and `label` values. The function should return the batch with tokenized strings padded to the same length along with the corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate(batch):\n",
    "    tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "    # todo: complete collate function\n",
    "    texts = [line['text'] for line in batch]\n",
    "    labels = torch.tensor([line['label'] for line in batch], dtype=torch.long).to(device)\n",
    "    tokenized = tokenizer(texts, padding='longest', return_tensors='pt')\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "    return {\n",
    "        'input_ids': input_ids.to(device),  # Move input_ids to device\n",
    "        'attention_mask': attention_mask.to(device),  # Move attention_mask to device\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate)\n",
    "val_dataloader = DataLoader(val_data, batch_size=8, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1037,  4942,  1011,  5675,  2594, 14308,  1999,  1996,  2227,\n",
      "          2000, 12348, 15138,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  8982,  2100,  5855,  7733,  2019,  4728, 26380,  4038,  1997,\n",
      "         10697,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  8040,  5668,  6810,  2987,  1005,  1056,  2507,  2149,  1037,\n",
      "          2839,  4276,  3228,  1037,  4365,  2055,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2143,  1005,  1055,  2754, 10419,  2015,  2024, 10305,\n",
      "          1998,  3568,  7782,  2121,  2084,  1996,  4728, 10174,  2396, 23664,\n",
      "          2008, 11859,  1998,  2058,  2860, 24546,  2015,  1996,  2143,  1005,\n",
      "          1055,  2537,  2640,  1012,   102],\n",
      "        [  101,  2009,  4858,  2049,  3185, 26966, 26552,  1999,  1996,  4714,\n",
      "          2824,  2008,  2071,  2191,  1037,  2711,  2040,  2038,  2973,  2014,\n",
      "          2166,  2431,  1011,  6680,  3402,  5256,  2039,  1998,  2202,  5060,\n",
      "          1012,   102,     0,     0,     0],\n",
      "        [  101,  1037, 23069,  6752,  2008,  2196,  7635,  2995,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  2198,  5529,  2368,  1005,  1055,  5896,  2003,  2440,  1997,\n",
      "         12511,  1010,  2048,  1011,  8789,  3494,  2040,  2024,  2505,  2021,\n",
      "         17075,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  3057,  2908,  3748,  1998,  2908,  2942,  2153,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='mps:0'), 'labels': tensor([0, 1, 0, 0, 1, 0, 0, 1], device='mps:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/kf68jy4547q1rjp1k8kg11_h0000gn/T/ipykernel_12154/3338605485.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "# check if DataLoader is as intended\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Fine-tune the model on the training dataset until you achieve at least 80% accuracy on the validation dataset. You are welcome to use the [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) or [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer, whichever you prefer. As always, you may need to experiment to find a good learning rate or to decide on other optimization hyperparameters like momentum.\n",
    "\n",
    "You should track and evaluate the training loss at least every hundred batches. Evaluate the validation loss and accuracy at least once every epoch of training. \n",
    "\n",
    "Note that you are working with a relatively large model and should expect a single epoch to take several minutes, even using GPU compute. This is one reason we direct you to evaluate the training loss at least every hundred batches to monitor progress. With well-chosen hyperparameters, you should only need a small number (such as 1-3) epochs of fine-tuning; this should take minutes but not hours.\n",
    "\n",
    "Make sure to use the `attention_mask`, else the BERT model will be encoding unecessary `[PAD]` characters at the ends of sequences within a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/kf68jy4547q1rjp1k8kg11_h0000gn/T/ipykernel_12154/3338605485.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 0.5324, Train Acc: 0.7212\n",
      "Batch 200, Loss: 0.4203, Train Acc: 0.7694\n",
      "Batch 300, Loss: 0.4024, Train Acc: 0.7867\n",
      "Batch 400, Loss: 0.3641, Train Acc: 0.7997\n",
      "Batch 500, Loss: 0.3901, Train Acc: 0.8055\n",
      "Batch 600, Loss: 0.3376, Train Acc: 0.8163\n",
      "Batch 700, Loss: 0.3389, Train Acc: 0.8234\n",
      "Batch 800, Loss: 0.3238, Train Acc: 0.8286\n",
      "Batch 900, Loss: 0.3373, Train Acc: 0.8318\n",
      "Batch 1000, Loss: 0.3449, Train Acc: 0.8334\n",
      "Epoch 1 - Val Loss: 0.3383, Val Acc: 0.8405\n",
      "Batch 100, Loss: 0.1353, Train Acc: 0.9575\n",
      "Batch 200, Loss: 0.1620, Train Acc: 0.9463\n",
      "Batch 300, Loss: 0.1895, Train Acc: 0.9404\n",
      "Batch 400, Loss: 0.1777, Train Acc: 0.9391\n",
      "Batch 500, Loss: 0.1578, Train Acc: 0.9403\n",
      "Batch 600, Loss: 0.1525, Train Acc: 0.9402\n",
      "Batch 700, Loss: 0.1608, Train Acc: 0.9400\n",
      "Batch 800, Loss: 0.1646, Train Acc: 0.9400\n",
      "Batch 900, Loss: 0.1846, Train Acc: 0.9383\n",
      "Batch 1000, Loss: 0.1825, Train Acc: 0.9367\n",
      "Epoch 2 - Val Loss: 0.4062, Val Acc: 0.8602\n",
      "Batch 100, Loss: 0.0420, Train Acc: 0.9862\n",
      "Batch 200, Loss: 0.0391, Train Acc: 0.9856\n",
      "Batch 300, Loss: 0.0787, Train Acc: 0.9812\n",
      "Batch 400, Loss: 0.0751, Train Acc: 0.9788\n",
      "Batch 500, Loss: 0.0921, Train Acc: 0.9765\n",
      "Batch 600, Loss: 0.0659, Train Acc: 0.9762\n",
      "Batch 700, Loss: 0.0949, Train Acc: 0.9752\n",
      "Batch 800, Loss: 0.0659, Train Acc: 0.9759\n",
      "Batch 900, Loss: 0.0790, Train Acc: 0.9747\n",
      "Batch 1000, Loss: 0.0604, Train Acc: 0.9751\n"
     ]
    }
   ],
   "source": [
    "# todo: fine-tune / train the modified BERT model for sentiment analysis\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels'].float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask).squeeze()\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).long()\n",
    "        correct += (predicted == labels.long()).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            avg_loss = total_loss / 100\n",
    "            print(f\"Batch {i+1}, Loss: {avg_loss:.4f}, Train Acc: {correct/total:.4f}\")\n",
    "            total_loss = 0.0\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels'].float()\n",
    "            outputs = model(input_ids, attention_mask).squeeze()\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).long()\n",
    "            val_correct += (predicted == labels.long()).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            \n",
    "    val_accuracy = val_correct / val_total\n",
    "    print(f\"Epoch {epoch+1} - Val Loss: {val_loss/len(val_dataloader):.4f}, Val Acc: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Finally, retrieve five examples (your choice) from the validation dataset for which your fine-tuned model made incorrect predictions. Interpret the results on these five examples. Do you think the model is clearly incorrect or is there any ambiguity in whether the reviews are positive or negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/kf68jy4547q1rjp1k8kg11_h0000gn/T/ipykernel_12154/3338605485.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: made for teens and reviewed as such, this is recommended only for those under 20 years of age... and then only as a very mild rental.\n",
      "True Sentiment: Positive\n",
      "Predicted Sentiment: Negative\n",
      "Text: those moviegoers who would automatically bypass a hip - hop documentary should give \" scratch \" a second look.\n",
      "True Sentiment: Positive\n",
      "Predicted Sentiment: Negative\n",
      "Text: byler is too savvy a filmmaker to let this morph into a typical romantic triangle. instead, he focuses on the anguish that can develop when one mulls leaving the familiar to traverse uncharted ground.\n",
      "True Sentiment: Positive\n",
      "Predicted Sentiment: Negative\n",
      "Text: there's absolutely no reason why blue crush, a late - summer surfer girl entry, should be as entertaining as it is\n",
      "True Sentiment: Positive\n",
      "Predicted Sentiment: Negative\n",
      "Text: in capturing the understated comedic agony of an ever - ruminating, genteel yet decadent aristocracy that can no longer pay its bills, the film could just as well be addressing the turn of the 20th century into the 21st.\n",
      "True Sentiment: Positive\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# todo: code for task 4 here\n",
    "incorrect_examples = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels'].float()\n",
    "        outputs = model(input_ids, attention_mask).squeeze()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).long()\n",
    "        for i in range(len(labels)):\n",
    "            if predicted[i] != labels[i]:\n",
    "                # makes an object that will be easier to manipulate later\n",
    "                incorrect_examples.append({\n",
    "                    \"text\": tokenizer.decode(input_ids[i], skip_special_tokens=True),\n",
    "                    \"true_label\": labels[i].item(),\n",
    "                    \"predicted_label\": predicted[i].item(),\n",
    "                    \"logit\": outputs[i].item()\n",
    "                })\n",
    "                \n",
    "        if len(incorrect_examples) >= 5:\n",
    "            break\n",
    "\n",
    "for ex in incorrect_examples:\n",
    "    print(f\"Text: {ex['text']}\")\n",
    "    print(f\"True Sentiment: {\"Positive\" if ex['true_label'] == 1 else \"Negative\"}\")\n",
    "    print(f\"Predicted Sentiment: {\"Positive\" if ex['predicted_label'] == 1 else \"Negative\"}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*briefly explain for task 4 here*\n",
    "\n",
    "For some of these examples, there is more ambiguity, while for others, there is not. Starting with the first example, the wording is quite ambiguous. As a human, I'm not even sure that I could honestly say that this comment is positive. For the second and third reviews, they are more clearly positive, but I can understand why there is some ambiguity stemming from words like \"automatically bypass\" and \"too savvy\". For the third review, phrases like \"absolutely no reason\" could lead to the negative prediction. Finally, the last review has phrases like \"comedic agony\" and \"can no longer pay its bills\", which have negative connotations on their own. So, I feel that all of these reviews have some degree of ambiguity, but a human would be able to properly classify most of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
