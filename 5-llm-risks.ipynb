{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuu3dLhAON6C"
   },
   "source": [
    "# Part 5: LLM Risks\n",
    "\n",
    "There is no programming in this part of the assignment. Instead, this part is about reading an influential paper on the possible risks and dangers of large language model training and deployment from a human and societal perspective. You will respond briefly to several reflection questions.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "Through this exercise, you will:\n",
    "1. **Read** about the possible societal risks of large language models. \n",
    "2. **Interpret** these risks and **articulate** your own view.\n",
    "\n",
    "To begin, read the 2021 article [*On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?*](https://dl.acm.org/doi/10.1145/3442188.3445922) by Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Schmitchell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLoY0i08PYDw"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Referencing the article but in your own words, articulate **at least two** societal challenges or risks assosciated with large language models. Explain in 1-2 paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Briefly explain for task 1 here*\n",
    "\n",
    "One societal challenge associated with LLMs is the harmful environmental impact. As mentioned in the article, the training process of a single BERT model on GPUs was estimated to use \"as much energy as a trans-American flight.\" Furthermore, training a Transformer model with neural architecture search was estimated to emit nearly 60 times as much CO2 as a human emits in a year. These statistics are almost unfathomable, and they provide context for just how much damage LLMs contribute to the environment. One might argue that these LLMs are pre-trained and this is not a repeated process. However, as advancements continue to be made to these types of models, they will be trained again and again, contributing more harm to the environment each time. If we want to keep leveraging the power of LLMs in our society, then we have to figure out how to mitigate the environmental impact, otherwise there will be no society left.\n",
    "\n",
    "Another societal challenge stemming from LLMs is encoding bias. This refers to the biases, such as \"stereotypical associations, or negative sentiment toward specific groups,\" that get included in training data and take form in harmful results. For example, researchers found that BERT assosciated \"phrases referencing persons with disabilities with more negative sentiment words,\" and phrases like gun violence, homelessness, and drug addiction were prevalent in texts discussing mental health. These implications are harmful as they push stereotypes that many people already have in their minds, and would only contribute to marginalization and stigmas in our society. Other articles have also dove into the harmful effects of biased training data in image classification tasks for crime, as race often plays far too large a role in these tasks and can skew results. If LLMs are going to stay an important part of our future society, then these biases must be eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "In Section 7, the article discusses Paths Forward. Briefly summarize the authors' suggestions. Do you believe these suggestions can mitigate the risks you identified in task 1? Why or why not? Explain in 1-2 paragraphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Briefly explain for task 2 here*\n",
    "\n",
    "I think that some of the suggestions made by the authors can help mitigate risks identified in task 1. Specifically, one suggestion was to acknowledge the high costs of training large models, and prioritize energy and compute efficiency to reduce economic inequities. I think that this sentiment is very forward-thinking and productive, as it is critical that researchers are taught to minimize the societal impact of their models. However, this may be a difficult policy to implement and actually stick to. It is easy for people to get ahead of themselves and prioritize technological growth and advancement even when it is actually moving our society backward overall. It is going to take more than awareness to ensure that efficiency is actually made a priority. Another suggestion the authors made was to responsibly collect and curate data. This means avoiding relying on large, indiscriminate datasets from easily scraped internet sources, and carefully curating data to ensure representativeness and reduce biases. This is crucial for LLMs, as people have come to rely on them for simple everyday tasks, and deeply ingrained biases in our models could cause serious damage in the long run. Furthermore, this policy is realistic. While it is more effort to curate and properly collect representative data, I think that it is doable. LLMs are not trained on biased data out of malintent, but rather just because it is easy and accessible. If awareness is spread about the harms of such methods, then I am sure that changes will be made to implement more equitable policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Since the publication of this paper, some scientists have [called for a pause](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) on further development and research on large language models. Take a position (agree or disagree) on the following claim: \"Large language model development is advancing too rapidly and the risks of such rapid development outweigh the short-term benefits.\" Explain why you hold your position in 1-2 paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyavXKOpQyFX"
   },
   "source": [
    "*Briefly explain for task 3 here*\n",
    "\n",
    "I agree that LLM development is advancing too rapidly, and that the risks of this development outweight the short-term benefits. The provided open letter from the Future of Life Institute outlines many of these risks that arise from excessively rapid model development. Specifically, models become more prone to spreading misinformation, displacing jobs, and biased decision-making. It is not true that all of these things will necessarily happen immediately, or even in the next couple of decades. However, if we continue our current rate of growth, the risk of such a future is very real, and it could be fruitful to take a short pause to re-evaluate the consequences of model development, and create a path forward that mitigates the risk of these harmful, unintended consequences. In this break, policymakers and researchers could spend time developing more regulatory frameworks and assess the many ethical concerns of this rapid model development. It would be a responsible decision for humanity to make before it is too late, and society is forced to suffer the consequences."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
